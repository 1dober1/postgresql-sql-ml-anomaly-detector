import pandas as pd
import numpy as np
import pickle
import os
import sys
import json
import psycopg
import requests
from datetime import datetime, timezone
from dotenv import load_dotenv

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.abspath(os.path.join(BASE_DIR, '..')))
load_dotenv(os.path.join(BASE_DIR, '..', '.env'))

# –§–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è (—Å—á–µ—Ç—á–∏–∫ –¥—Ä–µ–π—Ñ–∞)
STATE_FILE = os.path.join(BASE_DIR, "drift_state.json")
CONSECUTIVE_RUNS_LIMIT = 5  # 5 —Ä–∞–∑ –ø–æ–¥—Ä—è–¥ = –¥—Ä–µ–π—Ñ

try:
    from scripts.db_config import DB_CONFIG
    from scripts.train_model import train as train_model_emergency
except ImportError:
    from db_config import DB_CONFIG
    from train_model import train as train_model_emergency

TG_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TG_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")
MODEL_FILENAME = "model_baseline_v1.pkl"
MODEL_VERSION = "baseline_v1"

# –ü—Ä–∏–∑–Ω–∞–∫–∏ (—Å–æ–≤–ø–∞–¥–∞—é—Ç —Å train_model.py)
LOG_FEATURES = ['shared_read_per_call', 'temp_read_per_call', 'ms_per_row']
OTHER_NUM_FEATURES = ['calls_per_sec', 'cache_miss_ratio', 'temp_share', 'read_blks_per_row', 'exec_time_per_call_ms', 'rows_per_call', 'wal_bytes_per_call']
LEX_FEATURES = ['query_len_norm_chars', 'num_tokens', 'num_joins', 'num_where', 'num_group_by', 'num_order_by', 'has_write', 'has_ddl']
ALL_FEATURES = LOG_FEATURES + OTHER_NUM_FEATURES + LEX_FEATURES

# --- –§–ò–õ–¨–¢–† –°–ò–°–¢–ï–ú–ù–´–• –ó–ê–ü–†–û–°–û–í ---
SYSTEM_KEYWORDS = [
    'pg_catalog', 'information_schema', 'pg_toast', 'pg_stat_statements',
    'monitoring.', 'set application_name', 'show transaction isolation level',
    'begin', 'commit', 'rollback'
]

def is_system_query(text):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–∏—Å—Ç–µ–º–Ω—ã–π"""
    if not isinstance(text, str): return False
    text_lower = text.lower()
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º/—Å—Ö–µ–º–∞–º
    for kw in SYSTEM_KEYWORDS:
        if kw in text_lower:
            return True
            
    # 2. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ (–∫–æ—Ä–æ—Ç–∫–∏–µ –∫–æ–º–∞–Ω–¥—ã –¥—Ä–∞–π–≤–µ—Ä–æ–≤)
    if len(text_lower.strip()) < 10 and ('set' in text_lower or 'show' in text_lower):
        return True
        
    return False

# --- –†–ê–ë–û–¢–ê –° –°–û–°–¢–û–Ø–ù–ò–ï–ú ---
def load_state():
    if not os.path.exists(STATE_FILE):
        return {"bad_runs_streak": 0}
    try:
        with open(STATE_FILE, 'r') as f:
            return json.load(f)
    except:
        return {"bad_runs_streak": 0}

def save_state(state):
    with open(STATE_FILE, 'w') as f:
        json.dump(state, f)

# --- –£–¢–ò–õ–ò–¢–´ ---
def send_telegram_msg(text):
    if not TG_TOKEN or not TG_CHAT_ID:
        print("‚ö†Ô∏è Telegram config missing") 
        return
    url = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
    try:
        if len(text) > 4000: text = text[:4000] + "..."
        requests.post(url, data={"chat_id": TG_CHAT_ID, "text": text, "parse_mode": "HTML"})
    except Exception as e:
        print(f"‚ùå Failed to send Telegram: {e}")

def load_model():
    path = os.path.abspath(MODEL_FILENAME)
    if not os.path.exists(path):
        print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞! –û–±—É—á–∞–µ–º –Ω–æ–≤—É—é...")
        train_model_emergency()
    with open(path, 'rb') as f:
        return pickle.load(f)

def get_unscored_data():
    conn_str = f"postgresql+psycopg://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}"
    # –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ, —É –∫–æ—Ç–æ—Ä—ã—Ö –µ—â–µ –Ω–µ—Ç –æ—Ü–µ–Ω–∫–∏
    query = f"""
    SELECT v.*
    FROM monitoring.features_with_lex v
    LEFT JOIN monitoring.anomaly_scores s
      ON s.model_version = '{MODEL_VERSION}'
     AND s.window_end = v.window_end
     AND s.dbid = v.dbid AND s.userid = v.userid AND s.queryid = v.queryid
    WHERE s.window_end IS NULL
    ORDER BY v.window_end ASC
    LIMIT 2000; 
    """
    try:
        return pd.read_sql(query, conn_str)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ë–î: {e}")
        return pd.DataFrame()

def save_scores(df_results):
    query = """
    INSERT INTO monitoring.anomaly_scores (
        window_start, window_end, dbid, userid, queryid,
        model_version, anomaly_score, is_anomaly, reason, scored_at
    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
    ON CONFLICT (model_version, window_end, dbid, userid, queryid) DO NOTHING;
    """
    records = []
    now_ts = datetime.now(timezone.utc)
    for _, row in df_results.iterrows():
        records.append((
            row['window_start'], row['window_end'], row['dbid'], row['userid'], row['queryid'],
            MODEL_VERSION, float(row['anomaly_score']), bool(row['is_anomaly']),
            json.dumps(row.get('reason_json', {})), now_ts
        ))
    with psycopg.connect(**DB_CONFIG) as conn:
        with conn.cursor() as cur:
            cur.executemany(query, records)
            conn.commit()

def process_alerts(anomalies_df):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∞–Ω–æ–º–∞–ª–∏–∏, —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –†–ï–ê–õ–¨–ù–´–• (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö) –∞–ª–µ—Ä—Ç–æ–≤.
    """
    SCORE_THRESHOLD = 0.0 
    real_alerts_sent = 0
    
    with psycopg.connect(**DB_CONFIG) as conn:
        with conn.cursor() as cur:
            for _, row in anomalies_df.iterrows():
                if row['anomaly_score'] > SCORE_THRESHOLD: continue

                qid = row['queryid']
                # –î–æ—Å—Ç–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ –∞–ª–µ—Ä—Ç–∞
                try:
                    cur.execute("SELECT query_text FROM monitoring.query_lex_features WHERE queryid = %s LIMIT 1", (qid,))
                    res = cur.fetchone()
                    query_text = res[0] if res else "TEXT NOT FOUND"
                except: query_text = "ERR"

                # --- –§–ò–õ–¨–¢–†–ê–¶–ò–Ø ---
                if is_system_query(query_text):
                    # –¢–∏—Ö–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏
                    # print(f"üôà Skip system anomaly: {str(query_text)[:40]}...")
                    continue
                # ------------------

                msg = (
                    f"üö® <b>ANOMALY DETECTED</b>\n"
                    f"<b>Score:</b> {row['anomaly_score']:.3f}\n"
                    f"SQL: <code>{str(query_text)[:200].replace('<','&lt;')}</code>" 
                )
                print(f"üöÄ –û—Ç–ø—Ä–∞–≤–∫–∞ –∞–ª–µ—Ä—Ç–∞ (Score {row['anomaly_score']:.3f})")
                send_telegram_msg(msg)
                real_alerts_sent += 1
                
    return real_alerts_sent

# --- –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ---
def detect():
    # print(f"--- –ó–ê–ü–£–°–ö –î–ï–¢–ï–ö–¢–û–†–ê ({datetime.now().strftime('%H:%M:%S')}) ---")
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    new_df = get_unscored_data()
    if new_df.empty:
        # print("üí§ –ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.")
        return

    print(f"üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(new_df)} –Ω–æ–≤—ã—Ö –æ–∫–æ–Ω...")
    
    # 2. –ü—Ä–µ–¥–∏–∫—Ç (–û—Ü–µ–Ω–∏–≤–∞–µ–º –í–°–Å, –¥–∞–∂–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É)
    model = load_model()
    df_clean = new_df.copy()
    df_clean[ALL_FEATURES] = df_clean[ALL_FEATURES].fillna(0)
    
    X = df_clean[ALL_FEATURES]
    new_df['is_anomaly'] = (model.predict(X) == -1)
    new_df['anomaly_score'] = model.decision_function(X)
    
    # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ü–µ–Ω–∫–∏ –≤ –±–∞–∑—É
    save_scores(new_df)
    
    # 4. –ê–Ω–∞–ª–∏–∑ –¥–ª—è –∞–ª–µ—Ä—Ç–æ–≤ –∏ –¥—Ä–µ–π—Ñ–∞
    anomalies = new_df[new_df['is_anomaly'] == True]
    
    state = load_state()
    current_streak = state.get("bad_runs_streak", 0)
    
    if not anomalies.empty:
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∏ —à–ª–µ–º –∞–ª–µ—Ä—Ç—ã —Ç–æ–ª—å–∫–æ –ø–æ –¥–µ–ª—É
        real_alerts_count = process_alerts(anomalies)
        
        if real_alerts_count > 0:
            current_streak += 1
            print(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã —Ä–µ–∞–ª—å–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏. Streak: {current_streak}/{CONSECUTIVE_RUNS_LIMIT}")
        else:
            # –ê–Ω–æ–º–∞–ª–∏–∏ –±—ã–ª–∏, –Ω–æ –≤—Å–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ. –°—á–µ—Ç—á–∏–∫ –ù–ï —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º (–∏–ª–∏ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º).
            # –õ—É—á—à–µ —Å–±—Ä–æ—Å–∏—Ç—å, —Ç–∞–∫ –∫–∞–∫ –∞—Ç–∞–∫–∞ –ø—Ä–µ—Ä–≤–∞–ª–∞—Å—å –∏–ª–∏ –µ—ë –Ω–µ –±—ã–ª–æ.
            print("‚ÑπÔ∏è –ù–∞–π–¥–µ–Ω—ã —Ç–æ–ª—å–∫–æ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏. –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º.")
            current_streak = 0
    else:
        # –í—Å—ë —á–∏—Å—Ç–æ
        if current_streak > 0:
            print("‚úÖ –ù–∞–≥—Ä—É–∑–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–ª–∞—Å—å. –°–±—Ä–æ—Å —Å—á–µ—Ç—á–∏–∫–∞.")
        current_streak = 0

    # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –î–†–ï–ô–§–ê
    if current_streak >= CONSECUTIVE_RUNS_LIMIT:
        print(f"üõë –î–†–ï–ô–§ –ü–û–î–¢–í–ï–†–ñ–î–ï–ù! ({current_streak} –∑–∞–ø—É—Å–∫–æ–≤ –ø–æ–¥—Ä—è–¥ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∞–Ω–æ–º–∞–ª–∏—è–º–∏)")
        send_telegram_msg(f"üõë <b>SYSTEM DRIFT DETECTED</b>\n{current_streak} consecutive checks failed on USER queries.\nüîÑ Starting Retraining...")
        
        # –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (–æ–Ω–æ —Å–∞–º–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –±–ª–∞–≥–æ–¥–∞—Ä—è –∏–∑–º–µ–Ω–µ–Ω–∏—è–º –≤ train_model.py)
        train_model_emergency()
        
        current_streak = 0
        send_telegram_msg("‚úÖ Model successfully retrained.")

    # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    state["bad_runs_streak"] = current_streak
    save_state(state)

if __name__ == "__main__":
    detect()